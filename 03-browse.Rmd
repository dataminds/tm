# 둘러보기 

텍스트마이닝은 자료확보 -> 정제 -> 분석 - 소통 등의 구조로 이뤄져 있다. 각 단계별로 자세하게 다루기 이전에 모든 과정을 한번 체험해 보자. 

## 자료

텍스트데이터가 저장된 데이터구조는 다양하다. 벡터(vector), 데이터프레임(data frame), 리스트(list) 등의 구조에 저장해 사용한다. 가장 기본적인 텍스트데이터 구조는 문자형으로 구성된 문자벡터다. 

<논어> 학이편 1장의 텍스트데이터를 문자벡터에 담아보자.

```{r}
text_v <- c("배우면서도 때때로 익힌다면 또한 기쁘지 아니한가?",
            "벗이 먼 곳에서부터 오고 있다면 또한 즐겁지 아니한가?",
            "남이 알아주지 않아도 쌓아두지 않는다면 또한 군자가 아니겠는가?")
text_v
```




## 정제

### 정돈텍스트(tidy text)
문자형, 숫자형, 논리형 등의  데이터 유형 중 텍스트데이터유형은 문자형이다. 그런데 데이터 유형이 문자형이어서는 컴퓨터로 분석할 수 없다. 양화(quantification)시켜 컴퓨터가 계산할 수 있도록 바꿔줘야 한다. 

다양한 방법이 있는데, 데이터프레임을 '정돈된 데이터 원리(tidy data principle)'에 따라 만든 정돈텍스트(tidy text)구조부터 시작하자. 

정돈텍스트(tidy text)는 데이터프레임을 텍스트마이닝에 적합하도록 만든 데이터구조다. 일반적으로 데이터프레임은 복수의 열(column)로 이뤄져 있는데, 정돈텍스트 구조에서는 열을 단 하나로 고정시켰다. 즉, 행(row) 하나에 토큰(token)이 하나만 할당돼 있다. 토큰은 텍스트분석의 기본 단위다. 단어 하나를 토큰으로 이용하기도 하고, 복수의 단어(n-gram)를 묶어 하나의 토큰으로 이용하기도 한다. 



### 토큰화: `unnest_tokens` 함수 

정돈된 세계(tidyverse)에서 텍스트데이터를 정돈텍스트 구조에 담는 함수가 `tidytext`패키지에서 제공하는 `unnest_tokens`다. (철자에 주의하자. 복수 `s`가 붙어 있다.)

```{r eval=FALSE}
install.packages('tidytext')

```

`tidytext`의 자세한 사용법은 아래 사용설명서를 참조한다. 
 - tidytext https://cran.r-project.org/web/packages/tidytext/

`unnest_tokens`은 데이트프레임 구조의 데이터를 받아 처리한다. 앞서 만든 문자벡터 `text_v`를 정돈텍스트로 바꾸기 위해서는 먼저 데이터프레임으로 바꿔야 한다. `dplyr`페키지의 `tibble`함수를 이용해 `text_v`의 내용을 데이터프레임에 담아 보자. 티블(tibble)은 현대적인 데이터프레임의 형식이다. 문자열을 요인(factor)로 바꾸지 않는 등 텍스트분석에 사용하기 좋다. 

```{r}
library(tidyverse)
text_df <- tibble(line = 1:3, text = text_v)
text_df

```

이제 생성된 데이터프레임을 `tidytext`패키지의 `unnest_tokens`함수를 이용해 정돈텍스트 구조로 바꿔보자. 

```{r}
library(tidytext)
text_df %>% unnest_tokens(output = word, input = text)

```
 - input : 입력한 데이터프레임의 열 이름
 - output : 출력할 정돈텍스트의 열 이름 
 
이처럼 텍스트데이터를 컴퓨터가 분석할 수 있도록 양화할 수 있는 단위로 나누는 것은 토큰화(tkoenization)이라고 한다. 

이번에는 토큰을 단어 2개로 묶은 ngram으로 정돈텍스트를 만들어 보자. (철자에 주의. 복수 s)


```{r}
text_df %>% unnest_tokens(output = word, input = text, 
                          token = "ngrams", n = 2 )

```

`unnest_tokens`함수의 자세한 사용법은 `?unnest_tokens`의 도움말을 참고한다. 





## 분석

### 빈도: `count` 함수 

자 이제 텍스트마이닝을 해보자. 어떤 문서에서 많이 사용하는 단어가 있다면, 그 문서는 그 단어가 나타내는 의미에 의해 규정된다고 할 수 있다. '사랑'이란 단어를 많이 사용한 문서면 사랑에 대한 문서이고, '학습'이란 단어를 많이 사용했으면 학습에 대한 문서일 가능성이 높다. 

문서에 등장하는 단어를 세어 주는 함수가 `count`다. `count`함수를 앞서 정돈텍스트 구조에 저장된 텍스트에 많이 등장한 단어가 무멋인지 찾아보자. 

```{r}
text_td <- text_df %>% unnest_tokens(output = word, input = text)

text_td %>% 
  count(word, sort = TRUE)

```

문서의 양이 크지 않아 단어를 한번씩만 사용했다. 텍스트마이닝을 하기 위해서는 많은 양의 텍스트데이터가 필요하다는 것을 보여준다. 





## 소통

### 시각화 

### 한글 폰트 설정
```
library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()
```


